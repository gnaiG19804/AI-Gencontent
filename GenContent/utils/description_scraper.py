import requests
from bs4 import BeautifulSoup
import re
import sys
from pathlib import Path
from typing import List, Dict, Any

sys.path.insert(0, str(Path(__file__).parent.parent))
from config.config import Config
from utils.getPrice import build_search_query

SERP_KEY = Config.SERP_API_KEY

def get_competitor_links(product_name: str, vintage: str = None, limit: int = 3) -> List[str]:
    """
    T√¨m ki·∫øm s·∫£n ph·∫©m t∆∞∆°ng t·ª± d√πng Google Search (organic) ƒë·ªÉ c√≥ link tr·ª±c ti·∫øp.
    """
    query = build_search_query(product_name, vintage)
    # Th√™m t·ª´ kh√≥a ƒë·ªÉ t√¨m trang chi ti·∫øt s·∫£n ph·∫©m
    search_query = f"{query} product description review"
    
    params = {
        "engine": "google",
        "q": search_query,
        "api_key": SERP_KEY,
        "num": 10
    }

    try:
        r = requests.get("https://serpapi.com/search", params=params, timeout=15)
        print(f"üì° SerpAPI (Organic) Status: {r.status_code}")
        if r.status_code != 200:
            return []
        
        data = r.json()
        results = data.get("organic_results", [])
        print(f"üìä Found {len(results)} organic results from SerpAPI")
        
        links = []
        for item in results:
            link = item.get("link")
            title = item.get("title", "Unknown Title")
            
            # B·ªè qua c√°c trang m·∫°ng x√£ h·ªôi ho·∫∑c k·∫øt qu·∫£ kh√¥ng li√™n quan
            blacklist = ["facebook.com", "instagram.com", "twitter.com", "youtube.com", "google.com"]
            if link and not any(domain in link for domain in blacklist):
                links.append(link)
                print(f"‚úÖ Found organic link: {link} (Title: {title})")
            
            if len(links) >= limit:
                break
            
        return links
    except Exception as e:
        print(f"‚ùå Error fetching competitor links: {e}")
        return []

def scrape_description(url: str) -> str:
    """
    Truy c·∫≠p v√†o link v√† c·ªë g·∫Øng l·∫•y n·ªôi dung m√¥ t·∫£ s·∫£n ph·∫©m tinh vi h∆°n.
    """
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8",
            "Accept-Language": "en-US,en;q=0.9",
        }
        r = requests.get(url, headers=headers, timeout=12)
        if r.status_code != 200:
            print(f"   ‚ùå Status {r.status_code} for {url}")
            return ""
        
        soup = BeautifulSoup(r.text, 'html.parser')
        
        # Chi·∫øn thu·∫≠t l·∫•y description
        description = ""
        
        # 1. Th·ª≠ l·∫•y t·ª´ c√°c Selector ph·ªï bi·∫øn c·ªßa trang Wine/E-commerce
        potential_selectors = [
            ".pipProductDescription_content", # Wine.com
            ".product-description",
            ".product-details__description",
            ".view-more-text",
            ".short-description",
            "[data-test-id='product-description']",
            "#productDescription",
            "div[itemprop='description']",
            "section#description",
            "article" # C·ªë g·∫Øng l·∫•y nguy√™n ƒëo·∫°n text trong b√†i vi·∫øt
        ]
        
        for selector in potential_selectors:
            elem = soup.select_one(selector)
            if elem:
                text = elem.get_text(separator=' ', strip=True)
                if len(text) > 80: # Gi·∫£m ng∆∞·ª°ng xu·ªëng 80 ƒë·ªÉ l·∫•y ƒë∆∞·ª£c nhi·ªÅu h∆°n
                    description = text
                    print(f"   üéØ Found desc via selector: {selector} ({len(text)} chars)")
                    break

        # 2. N·∫øu v·∫´n qu√° ng·∫Øn, th·ª≠ l·∫•y Meta Description
        if len(description) < 80:
            meta_desc = (
                soup.find("meta", attrs={"name": "description"}) or 
                soup.find("meta", attrs={"property": "og:description"}) or
                soup.find("meta", attrs={"name": "twitter:description"})
            )
            if meta_desc:
                m_text = meta_desc.get("content", "").strip()
                if len(m_text) > len(description):
                    description = m_text
                    print(f"   üí° Found desc via Meta tags ({len(m_text)} chars)")

        # 3. Fallback cu·ªëi c√πng: L·∫•y text t·ª´ body n·∫øu v·∫´n ch∆∞a c√≥ g√¨ ƒë√°ng k·ªÉ
        if len(description) < 50:
            # Lo·∫°i b·ªè c√°c th·∫ª script, style, nav, footer ƒë·ªÉ l√†m s·∫°ch text
            for tag in soup(['script', 'style', 'nav', 'footer', 'header', 'aside']):
                tag.decompose()
            text = soup.get_text(separator=' ', strip=True)
            # L·∫•y 1 ƒëo·∫°n text d√†i ·ªü gi·ªØa body (th∆∞·ªùng l√† n·ªôi dung ch√≠nh)
            if len(text) > 100:
                description = text[:1200]
                print(f"   ‚ö†Ô∏è Fallback to raw body text ({len(description)} chars)")

        # Clean up text
        description = re.sub(r'\s+', ' ', description).strip()
        print(f"   ‚úÖ Final Scraped length: {len(description)} chars")
        
        return description[:1200]
    except Exception as e:
        print(f"   ‚ùå Error scraping {url}: {e}")
        return ""

async def get_competitor_context(product_name: str, vintage: str = None) -> str:
    """
    H√†m t·ªïng h·ª£p: T√¨m link -> Scrape -> Tr·∫£ v·ªÅ context cho LLM.
    S·ª≠ d·ª•ng snippet t·ª´ k·∫øt qu·∫£ t√¨m ki·∫øm l√†m fallback n·∫øu scrape th·∫•t b·∫°i.
    """
    query = build_search_query(product_name, vintage)
    search_query = f"{query} product description review"
    
    params = {
        "engine": "google",
        "q": search_query,
        "api_key": SERP_KEY,
        "num": 5
    }

    try:
        r = requests.get("https://serpapi.com/search", params=params, timeout=15)
        if r.status_code != 200:
            return ""
        
        data = r.json()
        results = data.get("organic_results", [])
        
        contexts = []
        for i, item in enumerate(results[:3]): # L·∫•y top 3
            link = item.get("link")
            snippet = item.get("snippet", "")
            title = item.get("title", "")
            
            print(f"üåê Processing similarity #{i+1}: {link}")
            
            # B·ªè qua n·∫øu l√† link google ho·∫∑c r√°c
            if not link or "google.com" in link:
                continue

            # Th·ª≠ scrape n·ªôi dung chi ti·∫øt
            desc = scrape_description(link)
            
            # Logic fallback: N·∫øu scrape kh√¥ng ra g√¨ ho·∫∑c b·ªã ch·∫∑n (403), d√πng snippet
            final_content = ""
            method = ""
            if desc and len(desc) > 100:
                final_content = desc
                method = "FULL SCRAPE"
            elif snippet and len(snippet) > 20:
                final_content = f"{title}: {snippet}"
                method = "SNIPPET FALLBACK"
            
            if final_content:
                contexts.append(f"--- Competitor {i+1} ({method}) ---\n{final_content}")
                print(f"   ‚úÖ Success using {method}")
            else:
                print(f"   ‚è≠Ô∏è Skipping competitor {i+1} (No content & no snippet)")
        
        if not contexts:
            return ""
        
        return "\n\n".join(contexts)
    except Exception as e:
        print(f"‚ùå Error in get_competitor_context: {e}")
        return ""

if __name__ == "__main__":
    # Test
    import asyncio
    async def test():
        context = await get_competitor_context("Chateau Margaux", "2018")
        print("\n=== CONTEXT FOUND ===\n")
        print(context)
    
    asyncio.run(test())
